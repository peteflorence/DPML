\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newlabel{toy1_train}{{1}{3}{Classification performance of a batch-gradient-descent-trained neural network on the training data itself for ``toy multiclass 1''. M = 30 hidden nodes were used, with $\lambda $ = 0, step size of 5e-5, and max 3,000 iterations. Shown classification error rate is 0.033 percent}{figure.1}{}}
\newlabel{toy1_val}{{2}{3}{Classification performance of a batch-gradient-descent-trained neural network on the validation data for ``toy multiclass 1''. M = 30 hidden nodes were used, with $\lambda $ = 0, step size of 5e-5, and max 3,000 iterations. Shown classification error rate is 0.033 percent}{figure.2}{}}
\newlabel{toy2_train}{{3}{3}{Classification performance of a batch-gradient-descent-trained neural network on the training data itself for ``toy multiclass 2''. M = 30 hidden nodes were used, with $\lambda $ = 0, step size of 5e-5, and max 3,000 iterations. Shown classification error rate is 0}{figure.3}{}}
\newlabel{toy2_val}{{4}{4}{Classification performance of a batch-gradient-descent-trained neural network on the validation data for ``toy multiclass 2'' . M = 30 hidden nodes were used, with $\lambda $ = 0, step size of 5e-5, and max 3,000 iterations. Shown classification error rate is 6.0 percent}{figure.4}{}}
\newlabel{batch_toydataset2}{{5}{4}{The classification error rate on the train, test and validation datasets for different numbers of hidden nodes. Performed with batch gradient descent, step size - 5e-5, max 3,000 iterations. Results shown are the average of ten trials}{figure.5}{}}
\newlabel{SGD_toy_data_2_CER}{{6}{4}{The SGD classification error rate on the train, test and validation datasets for different numbers of hidden nodes}{figure.6}{}}
\newlabel{SGD_toy_data_2_training_time}{{7}{4}{The training time using SGD for different numbers of hidden nodes}{figure.7}{}}
\newlabel{SGD_toy_2_val}{{8}{5}{Classification performance of SGD on the validation data for ``toy multiclass 2'' using $M = 25$ hidden nodes and $\lambda = 0$. The classification error rate is 8 percent}{figure.8}{}}
\newlabel{MNIST_batch_varyM}{{9}{5}{The classification error rate on the train, test and validation datasets for different numbers of hidden nodes for the MNIST dataset. Step size used was 1e-4, max iterations 5,000, and $\lambda =0$}{figure.9}{}}
\newlabel{MNIST_reg_batch}{{10}{5}{The classification error rate on the train, test and validation datasets for different regularization parameters, $\lambda $. Step size used was 1e-4, max iterations 5,000, and $M=150$}{figure.10}{}}
\newlabel{SGD_mnist_CER_hidden_nodes}{{11}{6}{Performance of SGD on the three datasets as a function of the number of hidden nodes. The number of iterations of SGD was set to $50,000$. For a given number of hidden nodes, 3 trials with random seeds were run and the best performance is shown above}{figure.11}{}}
\newlabel{SGD_mnist_training_time_hidden_nodes}{{12}{6}{Training time for stochastic gradient descent as a function of the number of hidden nodes. The number of iterations of SGD was set to $50,000$}{figure.12}{}}
\newlabel{SGD_minist_CER_lambda}{{13}{6}{Performance of SGD on the three datasets for different values of the regularize $\lambda $. The number of iterations was $70,000$ and we have $M = 60$ hidden nodes. The values of $\lambda $ that were evaluated are $\lambda = {0.5, 1, 2, 5, 10}$}{figure.13}{}}
